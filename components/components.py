# Everything will be in one file for now.
# When things start becoming very, very long, that's when we start splitting files off.
# Also, there will be a lot of comments in this code. I'm annotating pretty much every line I do so that I can learn what is going on.

import torch

from einops import rearrange
from torch import nn

from .activations import BIG_ASS_ACTIVATION_DICTIONARY as BAAD
from .position_embeddings import POS_EMBED_DICT

class Residual(nn.Module):
    """
    The residual. Applies a function to input x, and then re-adds the original x.
    TODO: Add residual scaling as implemented in x-transformers.
    """
    def __init__(self, fn):
        super().__init___()
        self.fn = fn
        
    def forward(self, x, **kwargs):
        return self.fn(x, **kwargs) + x

class ChosenActivation(nn.Module):
    """
    This is a wrapper for a chosen activation function that is in the Big-Ass Activation Dictionary (BAAD).
    """
    def __init__(self, activation: str):
        super().__init__
        self.activation = BAAD[activation]
        
    def forward(self, x):
        return self.activation(x)
        

class ChosenNorm(nn.Module):
    """
    Wrapper to normalize an input.
    For now, it'll just be standard nn.LayerNorm, but stuff like RMS norm should be implemented soon.
    """
    def __init__(self, dim, fn):
        super().__init__()
        self.norm = nn.LayerNorm(dim)
        self.fn = fn
        
    def forward(self, x, **kwargs):
        return self.fn(self.norm(x), **kwargs)
        
class Attention(nn.Module):
    """
    Multi-head attention. Lots of fun here.
    """
    def __init__(self, *, dim_model, pos_embedding, heads = 8, dim_heads = 64, dropout = 0.):
        super().__init__()
        self.num_heads = heads
        self.scale = dim_heads ** -0.5 # Scale of the dot product, 1 / sqrt(d_k)
        inner_dim = heads * dim_heads
        
        self.dropout = nn.Dropout(dropout)
        
        # Do a linear transformation on QKV vectors
        self.to_q = nn.Linear(dim_model, inner_dim, bias = False)
        # The reason this is to dim_head * 2 is because it will be split later into k and v by .chunk(2, dim=-1)
        self.to_kv = nn.Linear(dim_model, dim_heads * 2, bias = False)
        self.to_out = nn.Linear(inner_dim, dim_model)
        
        # TODO: Figure out how to dynamically grab required parameters for a certain positional embedding
        # We'll code in T5 manually for now.
        self.positional_embedding = POS_EMBED_DICT[pos_embedding](scale = dim_heads ** 0.5, heads = heads)
       
        
    def forward(self, x):
        # Using LaMDA code for now (which itself was probably taken from lucidrains), annotated. Will change later?
        heads = self.heads
        input_device = x.device
        
        # What this is doing:
        # Given input x, it generates both the query and key-value pair vectors.
        # q is generated by taking x and doing a linear transformation on it.
        # k and v are generated by taking x, doing a linear transform on it, and then chunking it into two on the last dimension.
        q, k, v = (self.to_q(x), *self.to_kv(x).chunk(2, dim = -1))
        
        # We are now calculating attention, where according to Attention is All You Need:
        # Attention(Q, K, V) = softmax(Q * K.T / self.scale) * V
        q = rearrange(q, 'b n (h d) -> b h n d', h = heads)
        q = q * self.scale
        
        # Now is the dot product between Q and K.T. labml.ai calls this calculating "attention scores", but no such term is found in the paper.
        # The dot product is done by an einsum of q and k - aka matrix multiplication.
        # It is referred to as "sim" in the original LaMDA repo.
        dot_products = torch.einsum('b h i d, b j d -> b h i j', q, k)
        # i and j represent the dimensions of the dot products for each head in the batch.
        i, j = dot_products.shape[-2:]
        
        # The dot products are now fed into the positional embeddings. In this hardcoded case, it's through T5's relative positional bias
        dot_products = self.positional_embedding(dot_products)
        
        # The casual mask is constructed. It is there to make sure that attention heads
        # are not dirty, filthy liars when they say they aren't cheating
        # by making connections with words they aren't supposed to have seen yet.
        casual_mask = torch.ones((i, j), dtype = torch.bool, device = device).triu(j - i + 1)
        # This sets any dot products of words that aren't supposed to be predicted yet to very near 0.
        dot_products = dot_products.masked_fill(casual_mask, -torch.finfo(dot_products.dtype).max)
        
        # Apply the softmax function.
        attention_output = dot_products.softmax(dim = -1)
        # Apply dropout, if there is any.
        attention_output = self.dropout(attention_output)
        
        # Finally, do matrix multiplication by V, the last part of the attention calculation.
        out = torch.einsum('b h i j, b j d -> b h i d', attention_output, v)
        # Rearrange back to input dims to prepare for entry into the FFN.
        out = rearrange(out, 'b h n d -> b n (h d)')
        return self.to_out(out)
        
class FeedForwardNetwork(nn.Module):
    """
    Feedforward network. Allows for plenty of customization... later.
    TODO: Make sure to handle GLU
    """
    def __init__(self, dim_model, activation, mult = 4, dropout = 0., num_linear_layers = 2):
        super().__init__()
        inner_dim = int(dim_model * mult)
        # Sanity check since inner dim is necessary
        assert num_linear_layers > 1, "Not enough linear layers in the feedforward network!"
        net = []
        # Last layer happens no matter how many come before it
        for layer in range(num_linear_layers - 1):
            is_first = layer == 0
            layer_dim_in = dim_model if is_first else inner_dim
            # Linear layer
            net.append(nn.Linear(layer_dim_in, inner_dim))
            net.append(activation)
        # Apply dropout and last layer
        net.append(nn.Dropout(dropout))
        net.append(nn.Linear(inner_dim, dim_model))
        # Construct network
        self.net = nn.Sequential(*net)
        
    def forward(self, x):
        return self.net(x)
        
class TransformerBlock(nn.Module):
    """
    The transformer block, consisting of an attention layer followed by a feedforward layer.
    It's simple for now, but will probably be more complex later.
    """
    # NOTE: Will kwargs work here?
    def __init__(self, dim_model, pos_embedding, activation, norm, **kwargs):
        super().__init__()
        # TODO: transform one "**kwargs" into two different kwargs for attention and ffn
        attention = Attention(dim_model, pos_embedding, **attn_kwargs)
        ffn = FeedForwardNetwork(dim_model, activation, **ffn_kwargs)
        self.block = nn.Sequential([
            Residual(ChosenNorm(dim_model, attention)),
            Residual(ChosenNorm(dim_model, ffn))
        ])
        
    def forward(self, x):
        return self.block(x)