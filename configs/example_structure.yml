# This default config serves as a template for other configs.
# It mirrors the LaMDA model for the most part, but with adjustments to it as done in the earlier LaMDA repo.
---
encoder: null # LaMDA is decoder-only

decoder:
    activation: squaredrelu
    attention:
        dim_head: 64
        num_heads: 4
    ffn:
        dropout: 0.0
        mult: 4
        num_linear_layers: 2
    pos_embedding: t5
    norm:
        norm_type: layernorm
        # Trying to implement pre-norm as optional right now makes me want to go to Norway
        # do_pre_norm: True
    transformer:
        embed_dim: 256
        num_layers: 5
    vocab_size: 32000
    
hyperparams:
    lr: 0.0001
...